<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Freddie's SEO Blog ://</title><link href="https://freddielarkins.xyz/" rel="alternate"></link><link href="https://freddielarkins.xyz/feeds/all.atom.xml" rel="self"></link><id>https://freddielarkins.xyz/</id><updated>2022-04-01T09:31:00+01:00</updated><subtitle>SEO, Python and other stuff.</subtitle><entry><title>Building a bin collection reminder bot in Python</title><link href="https://freddielarkins.xyz/building-a-bin-collection-reminder-bot-in-python.html" rel="alternate"></link><published>2022-04-01T09:31:00+01:00</published><updated>2022-04-01T09:31:00+01:00</updated><author><name>Freddie Larkins</name></author><id>tag:freddielarkins.xyz,2022-04-01:/building-a-bin-collection-reminder-bot-in-python.html</id><summary type="html">&lt;p&gt;Python is great for automating repetitive tasks, such as checking bin collection. I built a friendly Bin Bot to do just that.&lt;/p&gt;</summary><content type="html">&lt;!--Status: draft--&gt;

&lt;p&gt;&lt;strong&gt;Sunday nights in our house involve a weekly ritual.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, we remember bins are due the following morning. Second, we realise we've forgotten what was collected the previous week. Third, we check the council's &lt;a href="https://hackney-waste-pages.azurewebsites.net/"&gt;HackneyWaste bin-checker tool&lt;/a&gt; to see what bins are due.&lt;/p&gt;
&lt;p&gt;What if there was a better way?&lt;/p&gt;
&lt;p&gt;Well, with Python, there is!&lt;/p&gt;
&lt;p&gt;Using the &lt;a href="https://docs.python-requests.org/en/latest/"&gt;Requests&lt;/a&gt; library, we can mimic the series of http requests that your browser makes when using the HackneyWaste tool...&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gif showing the WasteChecker tool in DevTools" src="images/gifs/hackney-waste-portal.gif"&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;em&gt;Keep your eye on the DevTools panel on the left-hand side. Our precious requests!&lt;/em&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;...and we can get all the data you see rendered in your browser in JSON. We can use that data to send automatic reminders for what bins are being collected, alleviating the need to repeat the aforementioned ritual. Neat!&lt;/p&gt;
&lt;p&gt;You can check out the repo below:&lt;/p&gt;
&lt;div class="github-card" data-github="fredlarkins/hackney-bin-bot" data-width="400" data-height="" data-theme="default"&gt;&lt;/div&gt;
&lt;script src="//cdn.jsdelivr.net/github-cards/latest/widget.js"&gt;&lt;/script&gt;

&lt;p&gt;I'll run through some of the interesting challenges I when building the bot.&lt;/p&gt;
&lt;hr&gt;
&lt;div class="toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#challenges"&gt;Challenges&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#working-out-how-the-series-of-requests-fit-together"&gt;Working out how the series of requests fit together&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-sending-a-postcode-choosing-address"&gt;1. Sending a postcode / choosing address&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-retrieving-the-bin-collection-services-for-the-address"&gt;2. Retrieving the bin collection services for the address&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-retrieving-the-collection-schedules-for-each-waste-service"&gt;3. Retrieving the collection schedules for each waste service&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#setting-up-a-cron-job-to-send-bin-collection-alerts"&gt;Setting up a Cron Job to send bin-collection alerts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#learnings"&gt;Learnings&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-use-a-text-comparison-tool-to-tease-out-which-bits-of-request-bodies-responses-are-important"&gt;1. Use a Text Comparison tool to tease out which bits of request bodies / responses are important.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-throttle-devtools-to-slow-a-series-of-requests-down"&gt;2. Throttle DevTools to slow a series of requests down.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-if-you-cant-work-it-out-step-away-and-do-something-else"&gt;3. If you can't work it out, step away and do something else.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id="challenges"&gt;Challenges&lt;/h2&gt;
&lt;p&gt;When I first started on this project, I actually set out to build something in Selenium. I'd load up the HackneyWaste tool the in a headless browser and use a mixture of &lt;code&gt;browser.find_element()&lt;/code&gt; the &lt;code&gt;browser.send_keys()&lt;/code&gt; to run through the same flow that a real user would:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Input postcode&lt;/li&gt;
&lt;li&gt;Hit Enter on &lt;button&gt;Search for your address&lt;/button&gt;&lt;/li&gt;
&lt;li&gt;Select address from dropdown&lt;/li&gt;
&lt;li&gt;Hit Enter on &lt;button&gt;Go&lt;/button&gt;&lt;/li&gt;
&lt;li&gt;Scrape the resulting bin collection data using BeautifulSoup&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, I didn't feel that would really teach me much about how the application worked. Nor would I gain any experience in scraping data from the backend - as it were - rather than the frontend.&lt;/p&gt;
&lt;p&gt;So, I had to figure out how those series of requests led from this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Screenshot of the first screen on the Waste Collection tool" src="/images/webp/waste-services-screenshot.webp"&gt;&lt;/p&gt;
&lt;p&gt;To this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Screenshot of the final screen of the Waste Collection tool, showing collection dates" src="/images/webp/hackney-waste-final-screen.webp"&gt;&lt;/p&gt;
&lt;h3 id="working-out-how-the-series-of-requests-fit-together"&gt;Working out how the series of requests fit together&lt;/h3&gt;
&lt;p&gt;I've no shame in admitting it took me a while to get this right! &lt;/p&gt;
&lt;p&gt;I figured it would make sense to run through the flow as a user would, seeing which requests pertain to each step in the flow.&lt;/p&gt;
&lt;h4 id="1-sending-a-postcode-choosing-address"&gt;1. Sending a postcode / choosing address&lt;/h4&gt;
&lt;p&gt;The first couple of requests are &lt;code&gt;POST&lt;/code&gt; requests to the first server, &lt;code&gt;18.169.89.254&lt;/code&gt;. It receives the user's postcode and returns the street addresses for the user to select. &lt;/p&gt;
&lt;p&gt;&lt;code&gt;CTRL-F&lt;/code&gt; was by best friend here: I could see that the postcode was buried deep in the request body. When testing this first step with other postcodes, I could see that nothing else changed in the lines and lines of JSON - just the postcode.&lt;/p&gt;
&lt;p&gt;The response contains data for each of the addresses, such as street address and Basic Land and Property Unit class codes (I had to Google what those were - &lt;em&gt;tl;dr&lt;/em&gt; they're not relevant for bin collections).&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;relevant&lt;/em&gt; bit of information for each property is the &lt;code&gt;itemId&lt;/code&gt;. This alphanumeric code uniquely identifies each property:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;itemId: &amp;quot;5f898d4790478c0067f8c316&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The user selects their address from the dropdown list; the application then uses that home's &lt;code&gt;itemId&lt;/code&gt; in the next request to a second server, &lt;code&gt;3.10.72.124&lt;/code&gt;, to query what bin collections apply to that property.&lt;/p&gt;
&lt;h4 id="2-retrieving-the-bin-collection-services-for-the-address"&gt;2. Retrieving the bin collection services for the address&lt;/h4&gt;
&lt;p&gt;The next request is a &lt;code&gt;GET&lt;/code&gt; request to server 2 (used for all subsequent requests), asking for the bin collection services applicable to that specific address. Not all addresses in Hackney have collections, and collection days vary by street. &lt;/p&gt;
&lt;p&gt;The &lt;code&gt;itemID&lt;/code&gt; we just obtained is used in the endpoint for this &lt;code&gt;GET&lt;/code&gt; request:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://api.uk.alloyapp.io/api/item/&lt;b&gt;5f898d4790478c0067f8c318&lt;/b&gt;?token=67ad2108-dd2b-407a-849a-411a15adf0b1&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There is one field in the JSON response that we're interested in: &lt;code&gt;attributes_wasteContainersAssignableWasteContainers&lt;/code&gt;. This field contains a list of up to four new alphanumeric values, each of which represents a 'waste container' - i.e. recycling, food waste, black bins or garden waste. So far, the workflow looks a bit like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram of the series of requests made between client and 2x servers" src="/images/webp/hackney_step_1.webp"&gt;&lt;/p&gt;
&lt;p&gt;For each value returned, the browser makes a request using the same scheme above (appending the &lt;code&gt;id&lt;/code&gt; for the waste container to the endpoint) to map that value to a 'human-readable' service. So, for this property:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;5fa55c586b4fb500650caf08 ---&amp;gt; Recycling Sack
5faea1a108c64000672a88fe ---&amp;gt; Food Caddy (Small)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And so on.&lt;/p&gt;
&lt;h4 id="3-retrieving-the-collection-schedules-for-each-waste-service"&gt;3. Retrieving the collection schedules for each waste service&lt;/h4&gt;
&lt;p&gt;This next bit is a bit confusing, and seems to me somewhat inefficient. Anyway.&lt;/p&gt;
&lt;p&gt;The browser makes a &lt;code&gt;POST&lt;/code&gt; request for each &lt;code&gt;id&lt;/code&gt;  obtained above.&lt;/p&gt;
&lt;p&gt;What we want in return is the value for &lt;code&gt;attributes_scheduleCodeWorkflowID_5f8dbfdce27d98006789b4ec&lt;/code&gt;. This gives us the &lt;code&gt;id&lt;/code&gt; for the collection timetable, which we retrieve by hitting an endpoint &lt;em&gt;containing&lt;/em&gt; that timetable &lt;code&gt;id&lt;/code&gt; For instance:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;5fa55c586b4fb500650caf08    # the id for Recycling Sack
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Gives us:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;workflows_testWorkflowRoundM1Mon_5f8dea39e27d98006789b99f   # the id for the recycling collection schedule
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Incorporating the above &lt;code&gt;id&lt;/code&gt; in the endpoint for our &lt;code&gt;GET&lt;/code&gt; request gives us:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://api.uk.alloyapp.io/api/workflow/&lt;b&gt;workflows_testWorkflowRoundM1Mon_5f8dea39e27d98006789b99f&lt;/b&gt;?token=67ad2108-dd2b-407a-849a-411a15adf0b1&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;...which returns a JSON response with the timetable for that waste container:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;workflow&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;workflow&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Workflow_Round Mon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;enabled&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;trigger&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s2"&gt;&amp;quot;dates&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
                    &lt;span class="s2"&gt;&amp;quot;2020-04-13T00:00:00.000Z&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;// our timetable data&lt;/span&gt;
                    &lt;span class="s2"&gt;&amp;quot;2020-04-20T00:00:00.000Z&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s2"&gt;&amp;quot;2020-04-27T00:00:00.000Z&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="p"&gt;...&lt;/span&gt;
                    &lt;span class="s2"&gt;&amp;quot;2024-01-01T01:01:00.000Z&amp;quot;&lt;/span&gt;
                    &lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="c1"&gt;// and lots of other superfluous json...     &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Rinse and repeat for each waste container  (recycling, food waste etc.) available for that address.&lt;/p&gt;
&lt;p&gt;Represented visually, the flow looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram showing the second series of client/server requests" src="/images/webp/hackney_bin_service_steps_2-3.webp"&gt;&lt;/p&gt;
&lt;p&gt;It seems like there are lots of unnecessary requests in this flow. But who knows; I'm not one to tell developers how to do their jobs!&lt;/p&gt;
&lt;p&gt;Anyways, we've finally got what we want: the collection timetables for each waste service. But how to use that data to send alerts?&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="setting-up-a-cron-job-to-send-bin-collection-alerts"&gt;Setting up a Cron Job to send bin-collection alerts&lt;/h2&gt;
&lt;p&gt;I scratched my head about this one for a while as well.&lt;/p&gt;
&lt;p&gt;I settled on the solution of running a daily script as a &lt;a href="https://ostechnix.com/a-beginners-guide-to-cron-jobs/"&gt;Cron Job&lt;/a&gt; - basically a scheduled command - that loops through each timetable, subtracting every date listed from the present date. If the difference equals 1, the collection is the following day:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;collection_is_due_tomorrow&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="c1"&gt;# looping through the timetable, getting the difference between today&amp;#39;s date and the date in that iteration of the loop&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;date&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;timetable&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

        &lt;span class="c1"&gt;# if it&amp;#39;s the day before collection is due, this value will equal 1&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

            &lt;span class="c1"&gt;# append the waste container to the &amp;#39;collection_is_due_tomorrow&amp;#39; list&lt;/span&gt;
            &lt;span class="n"&gt;collection_is_due_tomorrow&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;waste_container&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The script composes a simple email including what is due the following day, and sends it to the email addresses provided when running &lt;code&gt;check-bins.py&lt;/code&gt; for the first time.&lt;/p&gt;
&lt;p&gt;This is the email I received last Sunday:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Screenshot of the email that the Bin Bot sends" src="/images/webp/bin-bot-email.webp"&gt;&lt;/p&gt;
&lt;p&gt;And that's it! The Bin Bot's got us covered.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="learnings"&gt;Learnings&lt;/h2&gt;
&lt;p&gt;I've done a bit of web scraping before, but this was the first time I tried to circumvent the front-end and go straight to the network requests to get the data I was after. So I'm definitely no expert. But I learned a fair bit from this exercise that'll hopefully serve me well in the future.&lt;/p&gt;
&lt;h3 id="1-use-a-text-comparison-tool-to-tease-out-which-bits-of-request-bodies-responses-are-important"&gt;1. Use a Text Comparison tool to tease out which bits of request bodies / responses are important.&lt;/h3&gt;
&lt;p&gt;At first, I was slightly overwhelmed by the JSON I was sending and receiving. I couldn't work out which bits mattered, and which stayed the same no matter what address I was using. Text Compare tools could help me isolate the important variables.&lt;/p&gt;
&lt;p&gt;Take these two identical looking &lt;code&gt;POST&lt;/code&gt;-request bodies:&lt;/p&gt;
&lt;p&gt;&lt;img alt="A text compare tool available online" src="/images/webp/text-compare.webp"&gt;&lt;/p&gt;
&lt;p&gt;Highlighted in blue, the only thing that is different between them is that very last value. From that, I know that bit of information is important, and likely explains why I get two different responses. You can use that approach with any two requests that look similar to zero in on the information that the server really needs.&lt;/p&gt;
&lt;h3 id="2-throttle-devtools-to-slow-a-series-of-requests-down"&gt;2. Throttle DevTools to slow a series of requests down.&lt;/h3&gt;
&lt;p&gt;Part of this challenge was understanding how a series of up to fifteen requests fit together. Slowing them down helped me to get a sense of what request was governing the visual data I could see trickling through on the page:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Location of the 'throttle' feature in DevTools" src="/images/webp/devtools-throtttling.webp"&gt;&lt;/p&gt;
&lt;h3 id="3-if-you-cant-work-it-out-step-away-and-do-something-else"&gt;3. If you can't work it out, step away and do something else.&lt;/h3&gt;
&lt;p&gt;I w̶a̶s̶t̶e̶d spent hours trying figure out how this application worked. Often, though, I found that I made a breakthrough when I'd stepped away from my laptop for a few hours or days. It's a bit of a cliché, but it really does help your brain reset and come at the project with a fresh set of eyes.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;P.S. Hackney Council: if you're reading this, please don't change your backend. My Bin Bot won't like it 🙃&lt;/em&gt;&lt;/p&gt;</content><category term="Python"></category><category term="Requests"></category><category term="JSON"></category></entry><entry><title>Using Text to Columns to analyse a site's traffic sources</title><link href="https://freddielarkins.xyz/using-text-to-columns-to-analyse-a-site-s-traffic-sources.html" rel="alternate"></link><published>2022-03-20T16:23:00+00:00</published><updated>2022-03-20T16:23:00+00:00</updated><author><name>Freddie Larkins</name></author><id>tag:freddielarkins.xyz,2022-03-20:/using-text-to-columns-to-analyse-a-site-s-traffic-sources.html</id><summary type="html">&lt;p&gt;Excel's Text to Columns feature is a quick and easy way to segment a list of URLs by subfolder to understand which parts of a site drive organic traffic.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Text to Columns is probably one of the tools I use most frequently in my day-to-day SEO work.&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;Kudos to Distilled's &lt;a href="https://www.distilled.net/excel-for-seo/"&gt;Excel for SEOs&lt;/a&gt; guide for inspiring this article – I'd highly recommend it to any SEOs looking to sharpen up their Excel skills. Anyway, let's get into it.&lt;/p&gt;
&lt;h2 id="how-to-use-text-to-columns"&gt;How to use Text to Columns&lt;/h2&gt;
&lt;p&gt;I've used an Ahrefs export for &lt;a href="https://www.hubspot.com/"&gt;HubSpot&lt;/a&gt; for this demo, but you can use Text to Columns for any dataset containing URLs or URL paths.&lt;/p&gt;
&lt;h3 id="1-format-your-data-as-a-table"&gt;1) Format your data as a table&lt;/h3&gt;
&lt;p&gt;Highlight your dataset with &lt;code&gt;CTRL+A&lt;/code&gt; and convert it into a table with CTRL+T.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Screenshot of the 'Create Table' pop-up" src="/images/png/create-table.png"&gt;&lt;/p&gt;
&lt;p&gt;(This step is optional – I find it makes the data easier to work with.)&lt;/p&gt;
&lt;h3 id="2-split-out-the-urls-using-text-to-columns"&gt;2) Split out the URLs using Text to Columns&lt;/h3&gt;
&lt;p&gt;Highlight the column containing your URLs. In the top Ribbon, click on &lt;code&gt;Data&lt;/code&gt; &amp;gt; &lt;code&gt;Text to Columns&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Screenshot of the location of the Text to Columns button in Excel" src="/images/png/text-to-columns-location.png"&gt;&lt;/p&gt;
&lt;p&gt;That'll bring up the Text to Columns Wizard. We'll run through the steps carefully to get the desired output.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; leave the Wizard as is and hit &lt;code&gt;Next &amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Screenshot showing step 1 of the Text to Columns Wizard" src="/images/png/text-to-columns-wizard-screenshot-step-1.png"&gt;&lt;/p&gt;
&lt;p&gt;The option to split our text (the URLs) by a delimiter – i.e. a specified text character – is selected by default.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; uncheck the default delimiter (Tab) and enter a forward slash in the &lt;code&gt;Other:&lt;/code&gt; input box.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Screenshot showing step 2 of the Text to Columns Wizard" src="/images/png/text-to-columns-wizard-screenshot-step-2.png"&gt;&lt;/p&gt;
&lt;p&gt;This tells the Text to Columns wizard to look for forward slashes as a way of 'splitting up' your URLs. You can also check &lt;code&gt;Treat consecutive delimiters as one&lt;/code&gt; to avoid an empty output column for the second slash in 'https:/&lt;strong&gt;/&lt;/strong&gt;'.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; select where the output will go.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Screenshot of step 3 of the Text to Columns wizard" src="/images/png/text-to-columns-wizard-screenshot-step-3.png"&gt;&lt;/p&gt;
&lt;p&gt;Choose the first cell in the first empty column next to your table. &lt;em&gt;Beware:&lt;/em&gt; if you proceed with Excel's default selection for the Destination cell, the newly-generated columns will overwrite whatever data you have in the columns to the right of your URLs.&lt;/p&gt;
&lt;p&gt;And we're done!&lt;/p&gt;
&lt;p&gt;I like to rename my columns to make analysis a little easier.&lt;/p&gt;
&lt;p&gt;I also tend to filter the first 'subfolder' column – &lt;code&gt;Path_1&lt;/code&gt; – for any blank cells and fill these in as 'homepage'.&lt;/p&gt;
&lt;h2 id="analysing-your-data-for-seo-insights"&gt;Analysing your data for SEO insights&lt;/h2&gt;
&lt;p&gt;You can use every SEO's best friend – a pivot table – to mine your data for useful insights. To turn your data into a pivot, highlight your table with &lt;code&gt;CTRL+A&lt;/code&gt; and hit &lt;code&gt;ALT+D+P&lt;/code&gt; to open up the PivotTable wizard, mashing &lt;code&gt;Enter&lt;/code&gt; until a pivot table appears in a new sheet.&lt;/p&gt;
&lt;p&gt;Let's see what we can find out about HubSpot.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The majority of traffic goes to HubSpot's blog subdomain&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To be precise, Ahrefs estimates that almost 91% of traffic goes to its blog. Kudos to HubSpot's content creation team!&lt;/p&gt;
&lt;p&gt;This pivot chart uses the 'Domain/subdomain' column generated by Text to Columns, aggregating traffic:
&lt;img alt="Pie chart showing traffic by subdomain" src="/images/png/traffic-by-subdomain.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HubSpot's blog traffic is driven by its /marketing/, /sales/ and /website/ subfolders&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This time, we've gone one level deeper to investigate the subfolders within &lt;a href="https://blog.hubspot.com/"&gt;https://blog.hubspot.com/&lt;/a&gt;. It seems that HubSpot's marketing content drives over half of of all traffic to its blog, followed by its Sales and Webmaster content.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Bar chart showing traffic by subfolder within the blog" src="/images/png/traffic-by-blog-subfolder.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Almost a quarter of blog traffic is driven by featured snippets&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I've turned this final pivot table into a Bar Graph via &lt;code&gt;Insert&lt;/code&gt; &amp;gt; &lt;code&gt;Recommended Charts&lt;/code&gt; &amp;gt; &lt;code&gt;Clustered Column&lt;/code&gt;. And wow! Almost 25% of the half a million monthly sessions to the blog come via a featured snippet. HubSpot are clearly masters at answering a user's primary question on a topic in a clear and concise manner.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Bar chart showing traffic to the blog by SERP feature" src="/images/png/traffic-by-serp-feature.png"&gt;
&lt;center&gt;&lt;em&gt;I get it - I'm terrible at Data Vis&lt;/em&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;There is lots more you can do with this dataset; we haven't even started incorporating keywords into our analysis yet!&lt;/p&gt;
&lt;p&gt;Feel free to &lt;a href="/excel-files/hubspot-t2c-analysis.xlsx"&gt;download the Excel spreadsheet&lt;/a&gt; used in this tutorial and have a go yourself!&lt;/p&gt;</content><category term="SEO"></category><category term="Excel"></category><category term="Text to Columns"></category></entry><entry><title>Identifying branded keywords in Excel</title><link href="https://freddielarkins.xyz/identifying-branded-keywords-in-excel.html" rel="alternate"></link><published>2022-03-20T15:59:00+00:00</published><updated>2022-03-20T15:59:00+00:00</updated><author><name>Freddie Larkins</name></author><id>tag:freddielarkins.xyz,2022-03-20:/identifying-branded-keywords-in-excel.html</id><summary type="html">&lt;p&gt;It’s always helpful to identify branded terms when you’re dealing with a list of keywords. Filtering them out gives you a view on non-branded performance and potential areas of improvement.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Any large-scale analysis of a group of keywords is always enhanced by identifying branded and non-branded keywords. Thankfully, it's simple to do in Excel.&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;div class="toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#the-formula"&gt;The formula&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-it-works"&gt;How it works&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#the-search-function"&gt;The SEARCH function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#the-isnumber-function"&gt;The ISNUMBER function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#the-if-function"&gt;The IF function&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id="the-formula"&gt;The formula&lt;/h2&gt;
&lt;p&gt;Assuming your keywords are in column A, this formula should do the trick:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;IF&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;ISNUMBER&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;SEARCH&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;BRANDNAME&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;,&lt;span class="nv"&gt;A2&lt;/span&gt;&lt;span class="ss"&gt;))&lt;/span&gt;,&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;Branded&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;,&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;Non-branded&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Just replace &lt;code&gt;BRANDNAME&lt;/code&gt; with the brand that you’re working, copy and paste the formula into Excel and apply it to the entire column of keywords. That’ll give you a column with either a 'Branded' or 'Non-branded' value for each keyword.~&lt;/p&gt;
&lt;h2 id="how-it-works"&gt;How it works&lt;/h2&gt;
&lt;p&gt;Let’s break down each part of the formula, working inside out. We’ll use an example set of Nike keywords to illustrate each step.&lt;/p&gt;
&lt;h3 id="the-search-function"&gt;The SEARCH function&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://exceljet.net/excel-functions/excel-search-function"&gt;SEARCH function&lt;/a&gt; looks for the provided string of text inside another &lt;a href="https://www.deskbright.com/excel/excel-string-functions/"&gt;string&lt;/a&gt; of text, returning the position of the first character in the case of a match. It’s not case sensitive. In our case, &lt;code&gt;=SEARCH("nike",A2)&lt;/code&gt; is asking Excel to look for the text “nike” inside our keyword text string.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Screenshot of an Excel table demonstrating the application of the SEARCH function to a set of keywords." src="/images/png/search-function-screenshot.png"&gt;&lt;/p&gt;
&lt;p&gt;If it is present, the formula will return the position of the “n” in “nike”. If it is not present, the formula will evaluate as a #VALUE! error.&lt;/p&gt;
&lt;h3 id="the-isnumber-function"&gt;The ISNUMBER function&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://exceljet.net/excel-functions/excel-isnumber-function"&gt;ISNUMBER function&lt;/a&gt; simply checks that a given cell is a number, returning either TRUE or FALSE. Thus, our formula becomes &lt;code&gt;=ISNUMBER(SEARCH("nike"),A2)&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Screenshot of an Excel table demonstrating the application of the SEARCH function in conjunction with ISNUMBER to a set of keywords." src="/images/png/search-and-isnumber-functions.png"&gt;&lt;/p&gt;
&lt;p&gt;By wrapping our &lt;code&gt;SEARCH&lt;/code&gt; function with &lt;code&gt;ISNUMBER&lt;/code&gt;, we’re converting the output of the former into a Boolean TRUE or FALSE value. In other words:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If “nike” is &lt;em&gt;present&lt;/em&gt; in keyword –&amp;gt; &lt;code&gt;SEARCH&lt;/code&gt; returns a number –&amp;gt; &lt;code&gt;ISNUMBER&lt;/code&gt; evaluates as TRUE&lt;/li&gt;
&lt;li&gt;If “nike” is &lt;em&gt;not present&lt;/em&gt; in keyword –&amp;gt; &lt;code&gt;SEARCH&lt;/code&gt; returns an #VALUE! error –&amp;gt; &lt;code&gt;ISNUMBER&lt;/code&gt; evaluates as FALSE&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Essentially, we’re converting the results of &lt;code&gt;SEARCH&lt;/code&gt; into something we can use in the final component of the formula, our &lt;code&gt;IF&lt;/code&gt; statement.&lt;/p&gt;
&lt;h3 id="the-if-function"&gt;The IF function&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://exceljet.net/excel-functions/excel-if-function"&gt;IF function&lt;/a&gt; uses a logical test to return one value for a TRUE result, and another for FALSE.&lt;/p&gt;
&lt;p&gt;We bring everything together using an &lt;code&gt;IF&lt;/code&gt; statement: &lt;code&gt;=IF(ISNUMBER(SEARCH("BRANDNAME",A2)),"Branded","Non-branded")&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Screenshot of an Excel table showing the full function" src="/images/png/search-isnumber-and-if-functions.png"&gt;&lt;/p&gt;
&lt;p&gt;Our logical test is the &lt;code&gt;ISNUMBER(...)&lt;/code&gt; component of the function. If that evaluates as TRUE, the &lt;code&gt;IF&lt;/code&gt; statement returns “Branded”. If FALSE, it returns “Non-branded”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;And that’s it!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I use this function without fail when I’m dealing with lists of keywords. After reading this, hopefully you will too.&lt;/p&gt;</content><category term="SEO"></category><category term="Excel"></category></entry><entry><title>Monitoring your site's most important URLs with Python</title><link href="https://freddielarkins.xyz/monitoring-your-site-s-most-important-urls-with-python.html" rel="alternate"></link><published>2022-03-19T15:09:00+00:00</published><updated>2022-03-19T15:09:00+00:00</updated><author><name>Freddie Larkins</name></author><id>tag:freddielarkins.xyz,2022-03-19:/monitoring-your-site-s-most-important-urls-with-python.html</id><summary type="html">&lt;p&gt;I wrote a few Python scripts that monitor your site's most important URLs for any 4xx and 5xx errors. Here's how they work.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;I've you've ever discovered a piece of content has been accidentally redirected or removed, this article might be for you.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As SEOs, we're precious about our site's traffic - especially when we're losing it! The inspo for this collection of Python scripts arose at work, where I'd noticed a few pieces of traffic-driving bits of content had been accidentally redirected elsewhere, or were stuck in infinite redirect loops. Of course, in both cases, traffic loss was the result.&lt;/p&gt;
&lt;p&gt;This &lt;a href="https://github.com/fredlarkins/monitor-top-urls"&gt;GitHub repo was&lt;/a&gt; my attempt to set up an automated monitoring system for a site's top URLs by organic clicks, checking daily for 4xx and 5xx errors and emailing you the results of the check.&lt;/p&gt;
&lt;div class="github-card" data-github="fredlarkins/monitor-top-urls" data-width="400" data-height="" data-theme="default"&gt;&lt;/div&gt;
&lt;script src="//cdn.jsdelivr.net/github-cards/latest/widget.js"&gt;&lt;/script&gt;

&lt;p&gt;If you just want to go ahead and try it, run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;git clone https://github.com/fredlarkins/monitor-top-urls.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;... and follow the README.md for set-up instructions.&lt;/p&gt;
&lt;p&gt;Otherwise, here's how it works in a little more detail.&lt;/p&gt;
&lt;h2 id="querying-the-search-console-api"&gt;Querying the Search Console API&lt;/h2&gt;
&lt;p&gt;The idea behind this project was to only monitor the site's most important URLs for downtime. Therefore, the script queries the Search Console API (GSC API) for a list of the top X URLs (where X is specified by the user when invoking the script) by organic clicks.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;query()&lt;/code&gt; function in &lt;code&gt;query_search_console.py&lt;/code&gt; does this job. Using Josh Carty's user-friendly &lt;a href="https://github.com/joshcarty/google-searchconsole"&gt;google-searchconsole&lt;/a&gt; package, it authenticates to the API with client_secret.json and client_config.json files - downloaded from the Google Developer Console. The important bit of the function is this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;webprop&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;\ &lt;span class="c1"&gt;# webprop is a &amp;#39;webproperty&amp;#39; object - just like the properties you see in the GSC GUI&lt;/span&gt;

        &lt;span class="c1"&gt;# asks the API for the last month of data&lt;/span&gt;
        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;today&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;months&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;\

        &lt;span class="c1"&gt;# aggregates the data by page&lt;/span&gt;
        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dimension&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;page&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;\

        &lt;span class="c1"&gt;# limits the number of URLs returned by the API to num_urls...&lt;/span&gt;
        &lt;span class="c1"&gt;# ... which is supplied by the user as a command-line argument&lt;/span&gt;
        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;limit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_urls&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;\

        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;\
        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_dataframe&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="c1"&gt;# return the result as a DataFrame&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Calling &lt;code&gt;query()&lt;/code&gt; supplies us with the DataFrame from which we'll get the list of URLs that are checked for errors.&lt;/p&gt;
&lt;h2 id="checking-the-urls-for-errors"&gt;Checking the URLs for errors&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;check_urls.py&lt;/code&gt; takes care of this part, leveraging the aiohttp library. For an awesome video on using aiohttp to make requests, check out John Watson Rooney's &lt;a href="https://youtu.be/lUwZ9rS0SeM"&gt;'Web Scraping with AIOHTTP and Python'&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are two really cool things about aiohttp. First, you can make requests asynchronously. aiohttp won't wait for the response from URL X before it requests URL Y - radically speeding up how many requests you can make in a given time period.&lt;/p&gt;
&lt;p&gt;Second, when you call &lt;code&gt;session.get(url)&lt;/code&gt; using an &lt;code&gt;aiohttpClientSession&lt;/code&gt; object, only the response headers are returned, rather than the full HTML contents. The latter can be obtained via the &lt;code&gt;text()&lt;/code&gt; method. It's a lightweight way of getting the information we need; after all, if we only want to know whether the URL is throwing an error or has been redirected, we only need the response codes from the server.&lt;/p&gt;
&lt;p&gt;Expressed in code, it looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt; &lt;span class="k"&gt;async&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;session&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;resp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;status_code&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;resp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;status&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;resolved_url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;resp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;
        &lt;span class="n"&gt;error_message&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;resp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;history&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;redirect_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;resp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;history&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;status&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;redirect_url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;resp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;history&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;redirect_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
                &lt;span class="n"&gt;redirect_url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Notice we're not calling &lt;code&gt;resp.text()&lt;/code&gt; at any point, as we don't need the HTML content to ascertain whether the URL is returning a 3xx or 4xx status code.&lt;/p&gt;
&lt;p&gt;Looping through each of the URLs obtained in the previous step, we'll record the URL, status code, error message (if applicable, e.g. in the case of a server error), redirect type, redirect URL and resolved URL. If any URLs throw errors, the user will be emailed in the next step.&lt;/p&gt;
&lt;h2 id="emailing-the-user-about-errors"&gt;Emailing the user about errors&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;app.py&lt;/code&gt; is where all the scripts are brought together. It uses the argparse library to take command-line arguments like the number of URLs to check and the recipients of the warning emails. It then runs through the flow outlined above, and uses a bit of conditional logic to send one of two email templates to the recipient: Errors Discovered, or No New Errors discovered.
&lt;img alt="Screenshot of a warning email" src="/images/png/errors-detected.png"&gt;
&lt;center&gt;&lt;em&gt;Oh no!&lt;/em&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;The emails are sent using the &lt;a href="https://pypi.org/project/yagmail/"&gt;yagmail&lt;/a&gt; package, a wonderfully simple SMTP client. Sadly, Gmail are retiring the option to allow less-secure-app-access to a Google account in summer '22; thereafter, sending emails via yagmail will require (I assume) some sort of OAuth implementation. So enjoy it while it lasts!&lt;/p&gt;
&lt;h2 id="learnings"&gt;Learnings&lt;/h2&gt;
&lt;p&gt;This was the first repo I made public on my GitHub, so it was quite exciting to release. I'm under no illusions that nobody will really use it, but it's a useful exercise to pretend as though people will! That forces you to focus on catching errors and communicating to the user why the script failed.&lt;/p&gt;
&lt;p&gt;Having said that, reading back through my code was a bit of a challenge. It's rather unwieldy and the end result could probably be achieved in one python file rather than several. It would also have been cool to have integrated a means of keeping track of &lt;em&gt;known&lt;/em&gt; errors. I noticed that it would remind me every day of the same problem URLs.&lt;/p&gt;
&lt;p&gt;Overall, though, I'm proud of it as my first attempt to give back to the SEO Pythonista community. 3 GitHub stars and counting!&lt;/p&gt;</content><category term="Python"></category><category term="Search Console API"></category><category term="aiohttp"></category><category term="yagmail"></category></entry></feed>